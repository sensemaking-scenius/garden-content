---
arweaveHash: 
author: Ronen Tamari
authorURL: 
bannerURI: https://storage.googleapis.com/papyrus_images/909a6a1c4499419e1096ea0ccbcffa83.jpg
date: 2024-07-31
license: 
subtitle: Why personal AI tools might not be all we need to revolutionize science
tags: 
title: From Genius Science to Scenius Science
uuid: 
publish: true
headDescription: 
headIcon: 
quartzShowCitation: true
quartzShowFlex: true
quartzShowGraph: true
quartzSearch: true
---
(cover image from Matt Akamatsu's [excellent talk at DeSci Denver 2024](https://www.youtube.com/watch?v=2xGQepp-f-8))

Hello sensemakers!

[Sensemaking Networks](https://sense-nets.xyz/) (or SenseNets) is a collective intelligence system designed to radically enhance information sharing and collaborative knowledge synthesis among researchers by integrating decentralized semantic web technology with AI and social networks. In our work on SenseNets over the past few months, we have come to notice a puzzle at the heart of how we do science.

The puzzle, which we will explore in this post, is that **many scientists value collaboration with other researchers, but the tools and metrics we use isolate us and promote competition**. Why is that?

To feel into this tension, imagine you are doing a literature survey of a new field, and have a stack of research papers in front of you. You have to decide which ones to read and in what order. To help you decide, you have access to assessments of each paper, but you need to choose between GPT-generated assessments or human assessments made by peers and experts in their field. Which would you choose?

![](https://storage.googleapis.com/papyrus_images/5fdcaad736e1cc175ad57e27d53fdf18.png)

I asked the audience this question during a [presentation of Sensemaking Networks](https://ronentk.github.io/talk/astera_may_24/slides.pdf) at an Open Science night organized by Astera in May. I was surprised by how _few_ people preferred the GPT option (hey, this is the Bay Area, after all ğŸ™ƒ).

These two choices (GPT and peer assessments) represent two different approaches to sensemaking in science: the former is focused on empowering **individual** researchers in isolation, while the latter focuses on empowering **collective, networked and social** aspects of sensemaking.

Sure, phrasing the question as a binary (either one or the other) is a little unfair. Indeed, we are most excited by [synergistic combinations of both approaches](https://paragraph.xyz/@sense-nets/2-project-plan). However, the story is a useful demonstration of the puzzle we opened with: people actually really value social sensemaking, and yet technology is overwhelmingly tailored to individual sensemaking. Personal AI research assistants are a dime a dozen, while tools that help researchers share information and think better together are few and far between. The very _lack_ of recognition for the role of social media in science is the [_raison d'Ãªtre_ of Sensemaking Networks](https://paragraph.xyz/@sense-nets/sense-nets-intro).

In fact, this isnâ€™t just the tech focus: academic research itself is also [primarily tailored towards augmenting individual researchers](https://arxiv.org/abs/2205.02007). [Recent research](http://arxiv.org/abs/2406.17809) even proposed developing a personalized â€œscience exocortexâ€ using a swarm of AI agents to automate an ever-growing range of scientific activities while leaving just the highest level decisions for the human operator.

Whatâ€™s gives? Why are we so focused on the individual side of science? And, given that we intuitively really value working with other humans, why donâ€™t we have more tools that improve collective sensemaking?

It turns out that there are a lot of intriguing answers to these questions. Weâ€™ll take a whirlwind tour through sociology, meta-science and collective intelligence theory, and reflect on what these diverse strands of research might imply for the future of SenseNets and science information systems more broadly.

## Sociology and lone genius mythology

Part of the answer lies in the â€œlone genius mythologyâ€ around science. We were all raised on stories of genius scientists who defied their doubters and against all odds made discoveries that advanced the frontier of human knowledge. These stories werenâ€™t wrong, of course; these scientists were extraordinarily talented and determined individuals. Yet the stories are also _incomplete_, for example omitting reference to other crucial (e.g., social) factors that contributed to the process of discovery. [An insightful critique](https://doi.org/10.1108/QROM-10-2023-2605) of the recent Hollywood portrayal of Robert Oppenheimer contrasts Hollywoodâ€™s â€œlone genius and his blackboard scienceâ€ with a perspective inspired by French sociologist Bruno Latour, according to whom â€œâ€™The father of the atomic bombâ€™ was no single parent, but rather a collective, networked oneâ€¦â€œ

In other words, the developers of sensemaking tools for individual researchers might be drawing more inspiration from Iron Man than from Latour.

![](https://storage.googleapis.com/papyrus_images/25bb204577fc4e1838b6d214c0654e80.png)

But the focus on individual researchers is not just a product of the stories we are told about scientists; there are more pervasive forces at play.

## Meta-science: Re-thinking the pecking order in science

A thought-provoking recent paper called â€œ[Shifting the Level of Selection in Science](https://doi.org/10.1177/17456916231182568)â€ shows how an overfocus on individuals is built into the very fabric of science, by way of the reward structures used to evaluate scientists:

> The predominant approach to scientific evaluation uses individual-level criteria, such as oneâ€™s number of first-authored publications, citations, h-indices, journal impact factors, and success in funding acquisitionâ€¦ This evaluation strategy implicitly assumes that identifying and rewarding the most accomplished individuals is the best way to generate scientific knowledge.

The authors show how individual-level criteria encourage competition and personally beneficial behavior among scientists. But, as they observe,

> personally beneficial behaviors are only a subset of the behaviors that benefit science. For example, the scientific community plausibly benefits from the open sharing of information such as code, materials, and raw data, whereas individual-level competition disincentivizes information-sharing to hinder competitorsâ€™ success

It turns out that we have a lot to learn from chicken farmers:

> the most productive hens in a coop are also the nastiest hens, feather-pecking and cannibalizing the other hens in their coop. Because individual hens who are most productive are those that harm others, selectively breeding the most productive hens can actually lead to lower overall egg production

Instead of selecting the most productive individuals, breeders have learned to select the most productive **groups.**

Perhaps science can do the same? The authors make a solid case for expanding evaluation mechanisms to incentivize group-level outcomes in addition to individual-level metrics. Importantly, such evaluation will require far greater recognition of diverse prosocial contributions, such as information sharing and other currently invisible â€œteam scienceâ€ roles.

So we have another piece of the puzzle - in a world where researchers are being incentivized to conceal rather than share their insights, private AI assistants are a safer bet for tool developers, even though social sharing platforms might actually be more effective in advancing science.

## Science as process, not product

Another fascinating line of research goes even more existential, in questioning the very aim of science. â€œShifting the Level of Selection in Scienceâ€ proposed new group-level incentives, but still focuses on a productivity-based model where science is about producing new knowledge. A recent piece called â€œ[An Epistemology for Democratic Citizen Science](https://royalsocietypublishing.org/doi/full/10.1098/rsos.231100)â€ makes a compelling case that sometimes the goal _is_ the journey in science. Rather than just viewing the knowledge as the product of science (what they call â€œindustrial scienceâ€), they make the case for re-thinking the social and cognitive processes that are generating scientific knowledge (â€ecological scienceâ€). Ecological science, beyond traditional scienceâ€™s role as an inquiry into the natural world, is also â€œan inquiry into how to best cultivate and utilise humanityâ€™s collective intelligenceâ€.

On this view, the social tools we use to communicate about science would themselves be at the core of the scientific process. Those tools would function as large scale experiments in collective intelligence (CI), informed by the latest research. Science Twitter is cool, but leaves much to be desired as a tool for researchers. We have a lot more to learn from CI theory; in â€œScience Communication as a Collective Intelligence Endeavorâ€, the authors provide an outline of what CI systems for science might look like. In particular, such systems would (a) enable better aggregation of distributed knowledge, (b) involve a more diverse group of contributors and (c) encourage increased public participation in science.

Connecting this research to our opening question, perhaps many tool developers are building â€œindustrial scienceâ€ tools for enhancing individual productivity, as opposed to â€œecological scienceâ€ tools for enhancing collective intelligence.

Industrial science, like industrial agriculture, risks creating scientific monocultures where

> some types of methods, questions and viewpoints come to dominate alternative approaches, making science less innovative and more vulnerable to errors.

In contrast, ecological science is more like permaculture, reflecting a diversity of methods, contributions and perspectives. See [Matt Akamatsuâ€™s talk at DeSci Denver](https://www.youtube.com/watch?v=2xGQepp-f-8) for an exciting example of what this might look like.

## Conclusion: from Genius Science to Scenius Science

For those of you who havenâ€™t heard of the term â€œsceniusâ€, musician-activist Brian Eno [coined it some 30 years ago](https://www.datadeluge.com/2019/01/brian-enos-definition-of-scenius-1996.html):

> **I became (and still am) more and more convinced that the important changes in cultural history were actually the product of very large numbers of people and circumstances conspiring to make something new. I call this â€˜sceniusâ€™ - it means â€˜the intelligence and intuition of a whole cultural sceneâ€™. It is the communal form of the concept of genius.**

Curiously, scenius has been around for a while but actually hasnâ€™t taken off:.

But given the dominant focus on individuals that weâ€™ve seen, maybe we shouldnâ€™t be too surprised at this point ğŸ¤·

Nonetheless, it feels like genius is fading and scenius is in the [_kairos_](https://en.wikipedia.org/wiki/Kairos). Converging evidence from across multiple fields suggests that if weâ€™re really serious about enhancing science for the benefit of humanity, we should be thinking less about how to find geniuses and a lot more about how to create sceniuses.

I'm excited to be working on my project in our own scenius in the making ğŸ—

Thanks to Kristen and Spencer for instigating this piece!